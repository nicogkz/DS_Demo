{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgakrelidz/opt/anaconda3/envs/llm-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "(…)d-sst-2-english/resolve/main/config.json: 100%|██████████| 629/629 [00:00<00:00, 852kB/s]\n",
      "model.safetensors: 100%|██████████| 268M/268M [00:03<00:00, 77.7MB/s] \n",
      "(…)glish/resolve/main/tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 441kB/s]\n",
      "(…)ned-sst-2-english/resolve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.45MB/s]\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp(\"I love burgers but I hate them cooked raw!\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'NEGATIVE', 'score': 0.9771497845649719}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp(\"Dear Madam, Dear sir, your service yesterday night was outstanding! Keep up the good work!\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'POSITIVE', 'score': 0.999870777130127}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "(…)distilled-squad/resolve/main/config.json: 100%|██████████| 473/473 [00:00<00:00, 1.99MB/s]\n",
      "model.safetensors: 100%|██████████| 261M/261M [00:03<00:00, 78.3MB/s] \n",
      "(…)squad/resolve/main/tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 31.5kB/s]\n",
      "(…)d-distilled-squad/resolve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 1.32MB/s]\n",
      "(…)tilled-squad/resolve/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 1.73MB/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r\"\"\"Mickey Mouse is an American cartoon character co-created in 1928 by Walt Disney and Ub Iwerks. The longtime icon and mascot of The Walt Disney Company, Mickey is an anthropomorphic mouse who typically wears red shorts, large yellow shoes, and white gloves. Inspired by such silent film personalities as Charlie Chaplin and Douglas Fairbanks, Mickey is traditionally characterized as a sympathetic underdog who gets by on pluck and ingenuity in the face of challenges bigger than himself.[2] The character's depiction as a small mouse is personified through his diminutive stature and falsetto voice, the latter of which was originally provided by Disney. Mickey is one of the world's most recognizable and universally acclaimed fictional characters of all time.\n",
    "\n",
    "Created as a replacement for a prior Disney character, Oswald the Lucky Rabbit, Mickey first appeared in the short Plane Crazy, debuting publicly in the short film Steamboat Willie (1928), while it was third cartoon featuring Mickey to be produced, it was also one of the first sound cartoons. The character was originally to be named \"Mortimer Mouse\", until Lillian Disney instead suggested \"Mickey\" during a train ride. The character went on to appear in over 130 films, including The Band Concert (1935), Brave Little Tailor (1938), and Fantasia (1940). Mickey appeared primarily in short films, but also occasionally in feature-length films. Ten of Mickey's cartoons were nominated for the Academy Award for Best Animated Short Film, one of which, Lend a Paw, won the award in 1941. In 1978, Mickey became the first cartoon character to have a star on the Hollywood Walk of Fame.\n",
    "\n",
    "Beginning in 1930, Mickey has also been featured extensively in comic strips and comic books. The Mickey Mouse comic strip, drawn primarily by Floyd Gottfredson, ran for 45 years. Mickey has also appeared in comic books such as Mickey Mouse, Disney Italy's Topolino and MM – Mickey Mouse Mystery Magazine, and Wizards of Mickey. Mickey also features in television series such as The Mickey Mouse Club (1955–1996) and others. He appears in other media such as video games as well as merchandising and is a meetable character at the Disney parks.\n",
    "\n",
    "Mickey generally appears alongside his girlfriend Minnie Mouse, his pet dog Pluto, his friends Donald Duck and Goofy, and his nemesis Pete, among others. Though originally characterized as a cheeky lovable rogue, Mickey was rebranded over time as a nice guy, usually seen as a spirited, yet impulsive hero.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nlp(question=\"Who is The Walt Disney Company mascot?\", context=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Mickey Mouse'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: '{a['answer']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nlp(question=\"How many movies The Walt Disney Company mascot appear?\", context=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'over 130'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: '{a['answer']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "(…)ingface.co/gpt2/resolve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 2.61MB/s]\n",
      "model.safetensors: 100%|██████████| 548M/548M [00:06<00:00, 86.5MB/s] \n",
      "(…)gpt2/resolve/main/generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 769kB/s]\n",
      "(…)gingface.co/gpt2/resolve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.89MB/s]\n",
      "(…)gingface.co/gpt2/resolve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.88MB/s]\n",
      "(…)face.co/gpt2/resolve/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 48.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "generate_text = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"Mickey Mouse is\", max_length=50, do_sample=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mickey Mouse is a game that is a bit of a mess. It's a game that is a bit of a mess. It's a game that is a bit of a mess. It's a game that is a bit of a mess.\n"
     ]
    }
   ],
   "source": [
    "print(res['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OMG! Let's try something different... Poor Mickey Mouse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"Mickey Mouse is a super hero but\", max_length=50, do_sample=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mickey Mouse is a super hero but he's also a villain. He's a villain because he's a villain because he's a villain because he's a villain because he's a villain because he's a villain because he's a villain because he\n"
     ]
    }
   ],
   "source": [
    "print(res['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Oh wow! Poor Mickey Mouse! Ok let's try something different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "(…)ilbart-cnn-12-6/resolve/main/config.json: 100%|██████████| 1.80k/1.80k [00:00<00:00, 9.14MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 1.22G/1.22G [00:14<00:00, 84.7MB/s]\n",
      "(…)-12-6/resolve/main/tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 98.5kB/s]\n",
      "(…)tilbart-cnn-12-6/resolve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 5.35MB/s]\n",
      "(…)tilbart-cnn-12-6/resolve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 5.78MB/s]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From wikipedia\n",
    "#https://en.wikipedia.org/wiki/Forrest_Gump\n",
    "\n",
    "text = r\"\"\"Forrest Gump is a 1994 American comedy-drama film directed by Robert Zemeckis and written by Eric Roth. It is based on the 1986 novel of the same name by Winston Groom and stars Tom Hanks, Robin Wright, Gary Sinise, Mykelti Williamson and Sally Field. The film follows several decades in the life of a slow-witted and kindhearted Alabama man named Forrest Gump (Hanks) and his experiences in the 20th-century United States. The film differs substantially from the novel.\n",
    "\n",
    "Principal photography took place between August and December 1993, mainly in Georgia, North Carolina and South Carolina. Extensive visual effects were used to incorporate Hanks into archived footage and to develop other scenes. The soundtrack features songs reflecting the different periods seen in the film.\n",
    "\n",
    "Forrest Gump was released in the United States on July 6, 1994, and received mostly positive reviews, with critical acclaim for Zemeckis's direction, performances (particularly those of Hanks and Sinise), visual effects, music, and screenplay. The film was an enormous success at the box office; it became the top-grossing film in the United States released that year and earned over US$678.2 million worldwide during its theatrical run, making it the second-highest-grossing film of 1994, behind The Lion King. The soundtrack sold over 12 million copies. Forrest Gump won six Academy Awards: Best Picture, Best Director, Best Actor for Hanks, Best Adapted Screenplay, Best Visual Effects, and Best Film Editing. It received many award nominations, including Golden Globes, British Academy Film Awards and Screen Actors Guild Awards.\n",
    "\n",
    "Various interpretations have been made of the protagonist and the film's political symbolism. In 2011, the Library of Congress selected the film for preservation in the United States National Film Registry as being \"culturally, historically, or aesthetically significant\".[3][4][5]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=summarizer(text, max_length=100, min_length=30, do_sample=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The film is based on the 1986 novel of the same name by Winston Groom . It stars Tom Hanks, Robin Wright, Gary Sinise, Sally Field and Robin Wright . The film was released in the U.S. on July 6, 1994, and received mostly positive reviews . It became the top-grossing film in the United States released that year and earned over US$678.2 million worldwide . The soundtrack sold over 12 million copies .\n"
     ]
    }
   ],
   "source": [
    "print(summary['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "(…)face.co/t5-base/resolve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 1.12MB/s]\n",
      "model.safetensors: 100%|██████████| 892M/892M [00:10<00:00, 87.1MB/s] \n",
      "(…)base/resolve/main/generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 120kB/s]\n",
      "(…)ace.co/t5-base/resolve/main/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 2.45MB/s]\n",
      "(…)e.co/t5-base/resolve/main/tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 3.43MB/s]\n",
      "/Users/nicolasgakrelidz/opt/anaconda3/envs/llm-env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Forrest Gump est un film de comédie-drame américain de 1994, rédigé par Robert Zemeckis et écrit par Eric Roth.'}]\n"
     ]
    }
   ],
   "source": [
    "print(translator(\"Forrest Gump is a 1994 American comedy-drama film directed by Robert Zemeckis and written by Eric Roth\", max_length=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\")\n",
    "\n",
    "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
    "           \"close to the Manhattan Bridge which is visible from the window.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-ORG', 'score': 0.9995635, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n",
      "{'entity': 'I-ORG', 'score': 0.99159384, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n",
      "{'entity': 'I-ORG', 'score': 0.99826705, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n",
      "{'entity': 'I-ORG', 'score': 0.9994404, 'index': 4, 'word': 'Inc', 'start': 13, 'end': 16}\n",
      "{'entity': 'I-LOC', 'score': 0.99943465, 'index': 11, 'word': 'New', 'start': 40, 'end': 43}\n",
      "{'entity': 'I-LOC', 'score': 0.99932706, 'index': 12, 'word': 'York', 'start': 44, 'end': 48}\n",
      "{'entity': 'I-LOC', 'score': 0.9993864, 'index': 13, 'word': 'City', 'start': 49, 'end': 53}\n",
      "{'entity': 'I-LOC', 'score': 0.9825622, 'index': 19, 'word': 'D', 'start': 79, 'end': 80}\n",
      "{'entity': 'I-LOC', 'score': 0.93698275, 'index': 20, 'word': '##UM', 'start': 80, 'end': 82}\n",
      "{'entity': 'I-LOC', 'score': 0.89870954, 'index': 21, 'word': '##BO', 'start': 82, 'end': 84}\n",
      "{'entity': 'I-LOC', 'score': 0.97582406, 'index': 29, 'word': 'Manhattan', 'start': 113, 'end': 122}\n",
      "{'entity': 'I-LOC', 'score': 0.99024945, 'index': 30, 'word': 'Bridge', 'start': 123, 'end': 129}\n"
     ]
    }
   ],
   "source": [
    "for e in nlp(sequence):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgakrelidz/opt/anaconda3/envs/llm-env/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1423: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "inputs = tokenizer.encode(\"translate English to German: Hugging Face is a technology company based in New York and Paris\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 11560,  3896,  8881,   229,   236,     3, 14366, 15377,   181,\n",
      "         11216,    16,   368,  1060,    64,  1919,     5,     1]])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "(…)tilroberta-base/resolve/main/config.json: 100%|██████████| 480/480 [00:00<00:00, 489kB/s]\n",
      "model.safetensors: 100%|██████████| 331M/331M [00:03<00:00, 83.4MB/s] \n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "(…)stilroberta-base/resolve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 8.09MB/s]\n",
      "(…)stilroberta-base/resolve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.75MB/s]\n",
      "(…)roberta-base/resolve/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 14.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nlp(f\"Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like {nlp.tokenizer.mask_token} and located in {nlp.tokenizer.mask_token} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.13847512006759644, 'token': 15157, 'token_str': ' ours', 'sequence': '<s>Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like ours and located in<mask> </s>'}, {'score': 0.0487150102853775, 'token': 33964, 'token_str': ' Dropbox', 'sequence': '<s>Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like Dropbox and located in<mask> </s>'}, {'score': 0.042071811854839325, 'token': 4318, 'token_str': ' mine', 'sequence': '<s>Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like mine and located in<mask> </s>'}, {'score': 0.03388786315917969, 'token': 15968, 'token_str': ' Cisco', 'sequence': '<s>Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like Cisco and located in<mask> </s>'}, {'score': 0.019363712519407272, 'token': 16315, 'token_str': ' Airbnb', 'sequence': '<s>Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like Airbnb and located in<mask> </s>'}]\n",
      "[{'score': 0.052385054528713226, 'token': 14415, 'token_str': ' NYC', 'sequence': '<s>Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like<mask> and located in NYC </s>'}, {'score': 0.048407796770334244, 'token': 20071, 'token_str': ' Bangalore', 'sequence': '<s>Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like<mask> and located in Bangalore </s>'}, {'score': 0.02982807345688343, 'token': 3417, 'token_str': ' Seattle', 'sequence': '<s>Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like<mask> and located in Seattle </s>'}, {'score': 0.026147741824388504, 'token': 4224, 'token_str': ' Austin', 'sequence': '<s>Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like<mask> and located in Austin </s>'}, {'score': 0.02246132120490074, 'token': 2920, 'token_str': ' Singapore', 'sequence': '<s>Nico is a data scientists with a business background and learned how to dev fullstack JS apps using NodeJS and React so he is a perfect fit for a company like<mask> and located in Singapore </s>'}]\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##outstanding! ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
