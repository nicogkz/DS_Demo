{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # actually weird, this is PyMuPDF...\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel,pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = '/Experiments/pdfs/ds_researches'\n",
    "query = \"summarizes details of individual LLM models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)cased/resolve/main/tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 30.1kB/s]\n",
      "(…)rt-base-uncased/resolve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 4.60MB/s]\n",
      "(…)bert-base-uncased/resolve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.68MB/s]\n",
      "(…)base-uncased/resolve/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.78MB/s]\n",
      "model.safetensors: 100%|██████████| 440M/440M [00:05<00:00, 83.2MB/s] \n"
     ]
    }
   ],
   "source": [
    "# let's try this pre-trained model \n",
    "modelName = \"bert-base-uncased\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModel.from_pretrained(modelName)\n",
    "\n",
    "# Extract text from PDFs\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    doc = fitz.open(pdf_file)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Compute embeddings yum!\n",
    "def compute_embeddings(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(input_ids).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = compute_embeddings(query, model, tokenizer)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Filename:1706.03762.pdf\n",
      "[+] Filename:2307.06435.pdf\n",
      "[+] Filename:TOIS_2020_HAL.pdf\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        print(\"[+] Filename:{}\".format(filename))\n",
    "        pdf_file = os.path.join(pdf_dir, filename)\n",
    "        pdf_text = extract_text_from_pdf(pdf_file)\n",
    "        pdf_embeddings = compute_embeddings(pdf_text, model, tokenizer)\n",
    "        similarity = cosine_similarity(query_embeddings, pdf_embeddings)\n",
    "        results.append((filename, similarity[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by Similarity\n",
    "results.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1706.03762.pdf', 0.6115461),\n",
       " ('2307.06435.pdf', 0.60113084),\n",
       " ('TOIS_2020_HAL.pdf', 0.59791964)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search Results:\n",
      "1. 1706.03762.pdf - Similarity: 0.6115\n",
      "2. 2307.06435.pdf - Similarity: 0.6011\n",
      "3. TOIS_2020_HAL.pdf - Similarity: 0.5979\n"
     ]
    }
   ],
   "source": [
    "most_similar_document = \"\"\n",
    "# Let's see how beautiful this is...\n",
    "print(\"Semantic Search Results:\")\n",
    "for i, (filename, similarity) in enumerate(results, 1):\n",
    "    print(f\"{i}. {filename} - Similarity: {similarity:.4f}\")\n",
    "    if i==1:\n",
    "        most_similar_document = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's define a couple of questions we would like to ask:\n",
    "\n",
    "questions = [\"What is the main topic of the document?\", \"Who is the author?\", \"When was it published?\", \"Why LLMs are so powerfull?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/ Question: What is the main topic of the document?\n",
      "Answer: journalistic or\n",
      "scholarly works\n",
      "\n",
      "1/ Question: Who is the author?\n",
      "Answer: Jürgen Schmidhuber\n",
      "\n",
      "2/ Question: When was it published?\n",
      "Answer: Input-Input Layer5\n",
      "\n",
      "3/ Question: Why LLMs are so powerfull?\n",
      "Answer: this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_file = os.path.join(pdf_dir, most_similar_document)\n",
    "pdf_text = extract_text_from_pdf(pdf_file)\n",
    "for i,question in enumerate(questions):\n",
    "    result = qa_pipeline(question=question, context=pdf_text)\n",
    "    print(f\"{i}/ Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hum, that will need a bit of rework..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
